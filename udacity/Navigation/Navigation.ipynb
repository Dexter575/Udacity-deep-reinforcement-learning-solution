{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 2.0.10 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This code is made using help from various sources like udacity and various people's implementation. I'll try to improve upon this code as i learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is something like this:\n",
    "\n",
    "1. `input_size = state_size = 37`\n",
    "2. 2 linear layers, one with 128 nodes as output and other with 64 nodes as output with relu activation for both\n",
    "3. A third linear layer with 4 outputs for 4 actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning algo to use\n",
    "\n",
    "The algo used in this project will be Q-learning algo with experience relay and fixed Q targets. The optimization of network will be done through `Adam` optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters used\n",
    "\n",
    "\n",
    "* `BUFFER_SIZE = int(1e5)` : replay buffer size\n",
    "* `BATCH_SIZE = 64` : minibatch size\n",
    "* `GAMMA = 0.99` : discount factor\n",
    "* `TAU = 1e-3` : for soft update of target parameters\n",
    "* `LR = 5e-4` : learning rate\n",
    "* `UPDATE_EVERY = 4` : how often to update the network\n",
    "* `EPS_DECAY=0.995` : the reduction factor of the epsilon-greedy policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Information\n",
    "\n",
    "The environment we will use is [Unity MLAgents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from dqn_agent import Agent\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll start the environment. To run on a new machine, before running the environment, change the `file_name` parameter to the path where the Unity environment is installed in the machine. \n",
    "\n",
    "Here are the various paths for different operating systems:\n",
    "\n",
    "\n",
    "* Mac: `\"path/to/Banana.app\"`\n",
    "* Windows (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "* Windows (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "* Linux (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "* Linux (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "* Linux (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "* Linux (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "The environment has 4 actions:\n",
    "`0`: walk forward\n",
    "`1`: walk backward\n",
    "`2`: turn left\n",
    "`3`: turn right\n",
    "\n",
    "State space has 37 dimensions. A reward of +1 will be awarded for collecting yellow banana and -1 for purple banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='/data/Banana_Linux_NoVis/Banana.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name] = env_info.vector_observations[0]\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=37, action_size=4, seed=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded episode number:  700\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "save_path = 'checkpoint.pth'\n",
    "episode_no = 1\n",
    "def save_model(model, episode_no):\n",
    "    if model:\n",
    "        dict_point = {'episode_no': episode_no, 'state_dict': model.state_dict()}\n",
    "        torch.save(dict_point, save_path)\n",
    "        print('Recently saved episode is' + str(episode_no), end='')\n",
    "\n",
    "def load_model(model):\n",
    "    if os.path.exists(save_path) and model:\n",
    "        dict_loaded = torch.load(save_path)\n",
    "        episode_no = dict_loaded['episode_no']\n",
    "        model.load_state_dict(dict_loaded['state_dict'])\n",
    "        print('Total loaded episode number is: ', episode_no)\n",
    "        return episode_no\n",
    "    return 1\n",
    "\n",
    "episode_no = load_model(agent.qnetwork_local)\n",
    "print(episode_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []                         #append scores from each episodes\n",
    "    scores_window = deque(maxlen = 100) # last hundred scores\n",
    "    eps = eps_start                     #initialize epsilon\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = (int)(agent.act(state, eps))\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            finish = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, finish)\n",
    "            state = next_state\n",
    "            score+=reward\n",
    "            if finish: \n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAvg Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end='')\n",
    "            save_model(agent.qnetwork_local, i_episode)\n",
    "        if np.mean(scores_window) >= 13.:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAvg Score: {:.2f}'.format(i_episode - 100,\n",
    "                                                                                         np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'model.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 785\tAverage Score: 15.05\n",
      "Environment solved in 685 episodes!\tAverage Score: 15.05\n"
     ]
    }
   ],
   "source": [
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-56792e08c418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the tested one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.0\n"
     ]
    }
   ],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('model.pth'))\n",
    "for i in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        finish = env_info.local_done[0]\n",
    "        score+=reward\n",
    "        state = next_state\n",
    "        if finish: \n",
    "            break\n",
    "\n",
    "    print(\"Episode No: {}\\t Score: {:.2f}\".format(i+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
